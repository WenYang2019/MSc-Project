{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "try:\n",
    "    import keras\n",
    "except:\n",
    "    import tensorflow.keras as keras\n",
    "print(keras.__version__)\n",
    "\n",
    "from keras.layers import Input, Dense, Conv1D, Conv2D, MaxPool1D, LSTM, Lambda, Concatenate, Reshape, Permute, Multiply, Add, Softmax, ReLU, Dropout, Flatten, Dot  \n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(auction, dataset_name, y_type, normalization_type, split_type, input_length):\n",
    "    \n",
    "    data=np.load(auction+'_'+dataset_name+'.npy')\n",
    "    break_points=[0]+list(np.load(auction+'_n_ticks.npy').T.flatten().cumsum())\n",
    "    print(data.shape)\n",
    "    y_type_dict={'y1':40,'y2':41,'y5':42,'y1true':43,'y2true':44,'y5true':45}\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    if not 'sample' in normalization_type:\n",
    "        # split data by stock and day\n",
    "        data=np.array([[data[break_points[stock*10+day]:break_points[stock*10+day+1]] for day in range(10)] \n",
    "                       for stock in range(5)])\n",
    "\n",
    "        if 'stationary' in normalization_type:\n",
    "            for stock in range(5):\n",
    "                for day in range(10):\n",
    "                    data[stock][day][:,:40]=data[stock][day][:,:40]-np.roll(data[stock][day][:,:40],1,axis=0)\n",
    "\n",
    "        normalization_factors=np.tile(np.stack((np.zeros([5, 10]),np.ones([5, 10])), axis=0), (2,1,1))\n",
    "        if normalization_type=='none':\n",
    "            pass\n",
    "        elif 'stock' in normalization_type and not 'day' in normalization_type:    \n",
    "            for stock in range(5):\n",
    "                training_data=np.vstack(data[stock][:5])\n",
    "                for i in range(2):\n",
    "                    normalization_factors[i*2+0, stock]=np.mean(training_data[:,range(i,40,2)])\n",
    "                    normalization_factors[i*2+1, stock]=np.std (training_data[:,range(i,40,2)])\n",
    "        elif 'day' in normalization_type:\n",
    "            for stock in range(5):\n",
    "                for day in range(10):\n",
    "                    training_data=data[stock][max(day-1,0)]\n",
    "                    for i in range(2):\n",
    "                        normalization_factors[i*2+0, stock, day]=np.mean(training_data[:,range(i,40,2)])\n",
    "                        normalization_factors[i*2+1, stock, day]=np.std (training_data[:,range(i,40,2)])     \n",
    "        print(normalization_factors)\n",
    "\n",
    "        for stock in range(5):\n",
    "            for day in range(10):\n",
    "                for field in range(2):\n",
    "                    data[stock][day][:, range(field,40,2)]-=normalization_factors[field*2+0, stock, day]\n",
    "                    data[stock][day][:, range(field,40,2)]/=normalization_factors[field*2+1, stock, day]\n",
    "                valid_ticks=np.arange(input_length, len(data[stock][day])-5)+1\n",
    "                x.append(np.expand_dims(data[stock][day][np.array([list(range(i-input_length,i)) for i in valid_ticks]), :40], axis=-1))\n",
    "                y.append(np.eye(3)[data[stock][day][valid_ticks-1,y_type_dict[y_type]].astype(int)-1])\n",
    "        x=np.array(x).reshape(5,10)\n",
    "        y=np.array(y).reshape(5,10)\n",
    "        \n",
    "    else:\n",
    "        # split data by stock and day\n",
    "        idx = np.concatenate([np.concatenate((np.flip(np.arange(i+2,40,4)), np.arange(i,40,4))) for i in range(2)])\n",
    "        data[:,:40]=data[:,:40][:,idx]\n",
    "        data=np.array([[data[break_points[stock*10+day]:break_points[stock*10+day+1]] for day in range(10)] \n",
    "                       for stock in range(5)])\n",
    "\n",
    "        for stock in range(5):\n",
    "            for day in range(10):\n",
    "                valid_ticks=np.arange(input_length, len(data[stock][day])-5)\n",
    "                x.append(data[stock][day][np.array([list(range(i-input_length,i+1)) for i in valid_ticks]), :40]\n",
    "                         .reshape(-1,input_length+1,2,20))\n",
    "                x[-1]=np.log10(x[-1]/(x[-1][:,:,:,9:11].mean(axis=(1,3)).reshape(-1,1,2,1)))\n",
    "                y.append(np.eye(3)[data[stock][day][valid_ticks,y_type_dict[y_type]].astype(int)-1])\n",
    "        x=np.array(x).reshape(5,10)\n",
    "        y=np.array(y).reshape(5,10)\n",
    "        \n",
    "        normalization_factors_mean=np.zeros([5,2,20])\n",
    "        normalization_factors_std =np.zeros([5,2,20])\n",
    "        for stock in range(5):\n",
    "            training_data=np.concatenate(x[stock][:5]).reshape(-1,2,20)\n",
    "            normalization_factors_mean[stock] = training_data.mean(axis=0)\n",
    "            normalization_factors_std [stock] = training_data.std (axis=0)\n",
    "\n",
    "        for stock in range(5):\n",
    "            for day in range(10):\n",
    "                x[stock][day]-=normalization_factors_mean[stock].reshape(1,1,2,20)\n",
    "                x[stock][day]/=normalization_factors_std [stock].reshape(1,1,2,20)\n",
    "                x[stock][day]=(x[stock][day]-np.roll(x[stock][day],1,axis=1))[:,1:]\n",
    "                x[stock][day]=x[stock][day].reshape(-1,input_length,40,1)[:,:,np.argsort(idx)]\n",
    "        \n",
    "    data={'x':x, 'y':y}      \n",
    "#     print(np.array([[len(data['x'][stock][day]) for day in range(10)] for stock in range(5)]))\n",
    "#     print(np.array([[len(data['y'][stock][day]) for day in range(10)] for stock in range(5)]))\n",
    "    \n",
    "    # split into train, val, and test sets\n",
    "    output={}\n",
    "    if split_type=='default':\n",
    "        output['train_x']=np.concatenate(np.array(data['x']).T[:5].flatten())\n",
    "        output['train_y']=np.concatenate(np.array(data['y']).T[:5].flatten())\n",
    "        output['val_x']  =np.concatenate(np.array(data['x']).T[5:7].flatten())\n",
    "        output['val_y']  =np.concatenate(np.array(data['y']).T[5:7].flatten())\n",
    "        output['test_x'] =np.concatenate(np.array(data['x']).T[7:].flatten())\n",
    "        output['test_y'] =np.concatenate(np.array(data['y']).T[7:].flatten())\n",
    "\n",
    "    _,axes=plt.subplots(1,3,figsize=(18,5), sharey=True)\n",
    "    for i, split in enumerate(['train', 'val', 'test']):\n",
    "        print(split, output[split+'_x'].shape, output[split+'_y'].shape)    \n",
    "        _=axes[i].plot(output[split+'_x'][:,0,0,0])\n",
    "    _=plt.show()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BL(x, D_prime, T_prime, D, T):\n",
    "    x = Conv2D(filters=D_prime, kernel_size=(D,1), activation=None,  use_bias=False)(x)\n",
    "    x = Permute((3,2,1))(x)\n",
    "    x = Conv2D(filters=T_prime, kernel_size=(1,T), activation='relu', use_bias=True)(x)\n",
    "    x = Permute((1,3,2))(x)\n",
    "    x = Dropout(rate=0.1)(x)\n",
    "    return x\n",
    "\n",
    "def TABL(x):\n",
    "    x = Permute((2,1,3))(x)\n",
    "    print(x.shape)\n",
    "    x = BL(x, D_prime=60, T_prime=10, D=40, T=10)\n",
    "    print(x.shape)\n",
    "    x = BL(x, D_prime=120, T_prime=5, D=60, T=10)\n",
    "    print(x.shape)\n",
    "    \n",
    "    x = Conv2D(filters=3, kernel_size=(120,1), activation=None, use_bias=False)(x)\n",
    "    x = Permute((3,2,1))(x)\n",
    "    e = Conv2D(filters=5, kernel_size=(1,5),   activation='softmax', use_bias=False, \n",
    "               kernel_initializer=keras.initializers.Constant(value=1/5))(x)  \n",
    "    e = Permute((1,3,2))(e)\n",
    "    x = Multiply()([x,e])\n",
    "    print(x.shape)\n",
    "    x = Conv2D(filters=1, kernel_size=(1,5),   activation=None, use_bias=True)(x)\n",
    "    print(x.shape)\n",
    "    x = Reshape((3,))(x)\n",
    "    x = Softmax(axis=-1)(x)\n",
    "    print(x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv_module_PriceVolume(x, n_filters, alpha, upper_threshold, lower_threshold):\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(1,2), strides=(1,2))(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(4,1), padding='same')(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(4,1), padding='same')(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "def Conv_module_LOBdepth(x, n_filters, alpha, upper_threshold, lower_threshold):\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(1,10))(x)\n",
    "    print(x.shape)\n",
    "    x = Lambda(lambda x: keras.backend.squeeze(x, 2))(x)\n",
    "    #   x = Reshape((100, n_filters))(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Conv1D(filters=n_filters, kernel_size=4, padding='same')(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Conv1D(filters=n_filters, kernel_size=4, padding='same')(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "def Inception_module(x, n_filters, alpha, upper_threshold, lower_threshold):\n",
    "    x1 = Conv1D(filters=n_filters, kernel_size=1, padding='same')(x)\n",
    "    x1 = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x1)\n",
    "    print(x1.shape)\n",
    "    x1 = Conv1D(filters=n_filters, kernel_size=3, padding='same')(x1)\n",
    "    x1 = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x1)\n",
    "    print(x1.shape)\n",
    "\n",
    "    x2 = Conv1D(filters=n_filters, kernel_size=1, padding='same')(x)\n",
    "    x2 = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x2)\n",
    "    print(x2.shape)\n",
    "    x2 = Conv1D(filters=n_filters, kernel_size=5, padding='same')(x2)\n",
    "    x2 = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x2)\n",
    "    print(x2.shape)\n",
    "\n",
    "    x3 = MaxPool1D(pool_size=3, strides=1, padding='same')(x)\n",
    "    print(x3.shape)\n",
    "    x3 = Conv1D(filters=n_filters, kernel_size=1, padding='same')(x3)\n",
    "    x3 = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x3)\n",
    "    print(x3.shape)\n",
    "\n",
    "    x = Concatenate(axis=-1)([x1,x2,x3])\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "def DeepLOB(x, alpha=0.01, upper_threshold=None, lower_threshold=0.0):\n",
    "    x = Conv_module_PriceVolume(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = Conv_module_PriceVolume(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = Conv_module_LOBdepth(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = Inception_module(x, 32, alpha, upper_threshold, lower_threshold)\n",
    "    x = LSTM(units=64, implementation=2, unroll=True)(x)\n",
    "    print(x.shape)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "    print(x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepLOB_clipped(x):\n",
    "    return DeepLOB(x, alpha=0.0, upper_threshold=1.0, lower_threshold=0.0)\n",
    "\n",
    "def Conv_module_LOBdepth_reversed(x, n_filters, alpha, upper_threshold, lower_threshold, p_filters=16, input_length=100):\n",
    "    x = Permute((2,1,3))(x)\n",
    "    print(x.shape)\n",
    "    x = Reshape((10,input_length*2,p_filters))(x)\n",
    "    print(x.shape)\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(10,1))(x)\n",
    "    print(x.shape)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Reshape((2,input_length,n_filters))(x)\n",
    "    print(x.shape)\n",
    "    x = Permute((2,1,3))(x)\n",
    "    print(x.shape)\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(4,1), padding='same')(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Conv2D(filters=n_filters, kernel_size=(4,1), padding='same')(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "def DeepLOB_reversed(x, alpha=0.01, upper_threshold=None, lower_threshold=0.0):\n",
    "    x = Conv_module_PriceVolume(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = Conv_module_LOBdepth_reversed(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = Conv_module_PriceVolume(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = Lambda(lambda x: keras.backend.squeeze(x, 2))(x)\n",
    "    x = Inception_module(x, 32, alpha, upper_threshold, lower_threshold)\n",
    "    x = LSTM(units=64, implementation=2, unroll=True)(x)\n",
    "    print(x.shape)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "def DeepLOB_combined(x, alpha=0.01, upper_threshold=None, lower_threshold=0.0):\n",
    "    x = Conv_module_PriceVolume(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x1 = Conv_module_PriceVolume(x, 8, alpha, upper_threshold, lower_threshold)\n",
    "    x1 = Conv_module_LOBdepth(x1, 8, alpha, upper_threshold, lower_threshold)\n",
    "    x2 = Conv_module_LOBdepth_reversed(x, 8, alpha, upper_threshold, lower_threshold)\n",
    "    x2 = Conv_module_PriceVolume(x2, 8, alpha, upper_threshold, lower_threshold)\n",
    "    x2 = Lambda(lambda x: keras.backend.squeeze(x, 2))(x2)\n",
    "    print(x1.shape,x2.shape)\n",
    "    x = Concatenate(axis=-1)([x1,x2])\n",
    "    print(x.shape)\n",
    "    x = Inception_module(x, 32, alpha, upper_threshold, lower_threshold)\n",
    "    x = LSTM(units=64, implementation=2, unroll=True)(x)\n",
    "    print(x.shape)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "def DeepLOB_noTimeConv(x, alpha=0.01, upper_threshold=None, lower_threshold=0.0):\n",
    "    x = Conv2D(filters=16, kernel_size=(1,2), strides=(1,2))(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Conv2D(filters=16, kernel_size=(1,2), strides=(1,2))(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Conv2D(filters=16, kernel_size=(1,10))(x)\n",
    "    print(x.shape)\n",
    "    x = Lambda(lambda x: keras.backend.squeeze(x, 2))(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Inception_module(x, 32, alpha, upper_threshold, lower_threshold)\n",
    "    x = LSTM(units=64, implementation=2, unroll=True)(x)\n",
    "    print(x.shape)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "def DeepLOB_noInception(x, alpha=0.01, upper_threshold=None, lower_threshold=0.0):\n",
    "    x = Conv_module_PriceVolume(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = Conv_module_PriceVolume(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = Conv_module_LOBdepth(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = LSTM(units=64, implementation=2, unroll=True)(x)\n",
    "    print(x.shape)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "def DeepLOB_noLSTM(x, alpha=0.01, upper_threshold=None, lower_threshold=0.0):\n",
    "    x = Conv_module_PriceVolume(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = Conv_module_PriceVolume(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = Conv_module_LOBdepth(x, 16, alpha, upper_threshold, lower_threshold)\n",
    "    x = Inception_module(x, 32, alpha, upper_threshold, lower_threshold)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    print(x.shape)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "def DeepLOB_doubled(x, alpha=0.01, upper_threshold=None, lower_threshold=0.0):\n",
    "    x = Conv_module_PriceVolume(x, 32, alpha, upper_threshold, lower_threshold)\n",
    "    x = Conv_module_PriceVolume(x, 32, alpha, upper_threshold, lower_threshold)\n",
    "    x = Conv_module_LOBdepth(x, 32, alpha, upper_threshold, lower_threshold)\n",
    "    x = Inception_module(x, 64, alpha, upper_threshold, lower_threshold)\n",
    "    x = LSTM(units=128, implementation=2, unroll=True)(x)\n",
    "    print(x.shape)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "    print(x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaled_Dot_Product_Attention(x, n_filters=16, alpha=0.01, upper_threshold=None, lower_threshold=0.0):\n",
    "    x1 = Conv1D(filters=n_filters, kernel_size=1)(x)\n",
    "    x1 = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x1)\n",
    "    x2 = Conv1D(filters=n_filters, kernel_size=1)(x)\n",
    "    x2 = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x2)\n",
    "    x3 = Conv1D(filters=n_filters, kernel_size=1)(x)\n",
    "    x3 = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x3)\n",
    "#     print(x3.shape)\n",
    "    x4 = Dot(axes=2, normalize=False)([x1,x2])\n",
    "#     print(x4.shape)\n",
    "    x4 = Lambda(lambda x: x/keras.backend.sqrt(keras.backend.cast(n_filters, 'float32')))(x4)\n",
    "    x4 = Softmax(axis=1)(x4)\n",
    "    x5 = Dot(axes=1, normalize=False)([x4,x3])\n",
    "#     print(x5.shape)\n",
    "    return x5\n",
    "\n",
    "def Multi_head_Attention(x, n_heads=8, n_filters=16, alpha=0.01, upper_threshold=None, lower_threshold=0.0):\n",
    "    x1 = [Scaled_Dot_Product_Attention(x, n_filters=n_filters, alpha=alpha, \n",
    "                                       upper_threshold=upper_threshold, lower_threshold=lower_threshold)\n",
    "          for i in range(n_heads)]\n",
    "    x = Concatenate(axis=-1)(x1)\n",
    "    print(x.shape)\n",
    "    return x\n",
    "\n",
    "def DMALOB(x, alpha=0.01, upper_threshold=None, lower_threshold=0.0):\n",
    "    x = Conv2D(filters=64, kernel_size=(1,2), strides=(1,2))(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Conv2D(filters=128, kernel_size=(1,2), strides=(1,2))(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Conv2D(filters=128, kernel_size=(1,10))(x)\n",
    "    x = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(x)\n",
    "    print(x.shape)\n",
    "    x = Lambda(lambda x: keras.backend.squeeze(x, 2))(x)\n",
    "    print(x.shape)\n",
    "    x = Multi_head_Attention(x, alpha=alpha, upper_threshold=upper_threshold, lower_threshold=lower_threshold)\n",
    "    x = Multi_head_Attention(x, alpha=alpha, upper_threshold=upper_threshold, lower_threshold=lower_threshold)\n",
    "    x = Lambda(lambda x: x[:,-1])(x)\n",
    "    print(x.shape)\n",
    "    x = Dense(64)(x)\n",
    "    print(x.shape)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "    print(x.shape)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPU_setup(gpu_number, thread_number, allow_growth=True, allow_soft_placement=True):\n",
    "    tf.reset_default_graph()\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    tf_config = tf.ConfigProto(allow_soft_placement=allow_soft_placement)\n",
    "    tf_config.gpu_options.visible_device_list = str(gpu_number)\n",
    "    tf_config.gpu_options.per_process_gpu_memory_fraction = 1/thread_number\n",
    "    tf_config.gpu_options.allow_growth = allow_growth\n",
    "    keras.backend.set_session(tf.Session(config=tf_config) )\n",
    "    \n",
    "def Train_model(train_x, train_y, val_x, val_y, model_name, lr, batch_size, output_folder, class_weight):\n",
    "    models_list={'TABL':TABL, 'DeepLOB':DeepLOB,\n",
    "                 'DeepLOB_clipped':DeepLOB_clipped, 'DeepLOB_reversed':DeepLOB_reversed, \n",
    "                 'DeepLOB_combined':DeepLOB_combined, 'DeepLOB_noTimeConv':DeepLOB_noTimeConv,\n",
    "                 'DeepLOB_noInception':DeepLOB_noInception, 'DeepLOB_noLSTM':DeepLOB_noLSTM,\n",
    "                 'DeepLOB_doubled':DeepLOB_doubled, 'DMALOB': DMALOB}\n",
    "    epsilon=None if 'TABL' in model_name else 1\n",
    "    start=time.time()\n",
    "    inputs = Input(shape=train_x.shape[1:])\n",
    "    outputs = models_list[model_name](inputs)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['categorical_accuracy'], \n",
    "                  optimizer=keras.optimizers.Adam(lr=lr, epsilon=epsilon))\n",
    "\n",
    "    earlystopping = keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=10, \n",
    "                                                  verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
    "    csvlogger = keras.callbacks.CSVLogger(output_folder+'history.csv', separator=',', append=False)\n",
    "    history = model.fit(train_x, train_y, validation_data=(val_x, val_y), batch_size=batch_size, epochs=200, \n",
    "                        shuffle=True, verbose=1, callbacks=[earlystopping, csvlogger], class_weight=class_weight) \n",
    "    \n",
    "    model.save(output_folder+'model.hdf5')   \n",
    "    \n",
    "    training_time = int((time.time()-start)/60)\n",
    "    print('training time used:', training_time//60, ':' , training_time%60)\n",
    "    \n",
    "    return model, history.history, training_time\n",
    "\n",
    "def Plot_training_curve(history):\n",
    "    _, axes=plt.subplots(1,2,figsize=(20,5))\n",
    "    for i, metric in enumerate(['loss', 'categorical_accuracy']):\n",
    "        for split in ['', 'val_']:\n",
    "            _=axes[i].plot(history[split+metric], label=split+metric)\n",
    "        _=axes[i].legend()\n",
    "    plt.show()\n",
    "\n",
    "def Get_test_results(model, test_x, test_y):\n",
    "    predict_test_y=np.eye(test_y.shape[1])[np.argmax(model.predict(test_x, batch_size=2048, verbose=1), axis=1)]\n",
    "    results = {'accuracy ': accuracy_score(test_y, predict_test_y)}\n",
    "    results.update(zip(['precision', 'recall', 'f1_score'], precision_recall_fscore_support(test_y, predict_test_y, average='weighted')[:3]))\n",
    "\n",
    "    print('\\n'.join([item[0]+' '.join(['' for i in range(12-len(item[0]))])+str(item[1]) for item in results.items()]))\n",
    "    print(confusion_matrix(np.argmax(test_y, axis=1), np.argmax(predict_test_y, axis=1)))\n",
    "    print(precision_recall_fscore_support(test_y, predict_test_y, average=None))\n",
    "    return results\n",
    "\n",
    "def Save_information(preprocessing_parameters, training_parameters, results, training_time, history, output_folder):\n",
    "    information={**preprocessing_parameters, **training_parameters, **results}\n",
    "    information['training_time']=training_time\n",
    "    information['epochs_trained']=len(history['loss'])\n",
    "    information=json.dumps(information, separators=('\\n ', ': '), sort_keys=False)\n",
    "    print(information)\n",
    "    with open(output_folder+'information.json', 'w') as f:\n",
    "        f.write(information)\n",
    "        \n",
    "def Load_model(output_folder):\n",
    "    model = keras.models.load_model(output_folder+'model.hdf5', custom_objects={'keras': keras})\n",
    "    return model\n",
    "\n",
    "def Load_information(output_folder):\n",
    "    information=json.loads(open(output_folder+'information.json', 'r').read().replace('\\n ', ', '))\n",
    "    print(json.dumps(information, separators=('\\n ', ': '), sort_keys=False))\n",
    "    return information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for y_type in ['y1true', 'y2true', 'y5true']:\n",
    "    for normalization_type in ['none', 'stock-stationary']:\n",
    "        model_name = 'DeepLOB'\n",
    "        \n",
    "        if 'TABL' in model_name:\n",
    "            batch_size, lr, input_length = (256, 0.001, 10)\n",
    "#             auction, dataset_name = ('Auction', 'Zscore')\n",
    "        elif 'LOB' in model_name:\n",
    "            batch_size, lr, input_length = (32, 0.01, 100)\n",
    "#             auction, dataset_name = ('NoAuction', 'Zscore')\n",
    "            \n",
    "        if normalization_type == 'none':\n",
    "            auction, dataset_name = ('NoAuction', 'Zscore')\n",
    "        else:\n",
    "            auction, dataset_name = ('NoAuction', 'DecPre')\n",
    "        \n",
    "        weighted = True\n",
    "        class_weight=None\n",
    "        if weighted==True:\n",
    "            if y_type=='y1true':\n",
    "                class_weight={0:7.0, 1:1.0, 2:7.0}\n",
    "            elif y_type=='y2true':\n",
    "                class_weight={0:3.5, 1:1.0, 2:3.5}\n",
    "            elif y_type=='y5true':\n",
    "                class_weight={0:1.3, 1:1.0, 2:1.3}\n",
    "                \n",
    "        output_folder=os.getcwd()+'/output_'+'_'.join(['weighted', y_type, model_name, normalization_type])+'/'\n",
    "        \n",
    "        preprocessing_parameters={'auction':auction, 'dataset_name':dataset_name, 'y_type':y_type,\n",
    "                                  'normalization_type':normalization_type, 'split_type':'default'}\n",
    "        training_parameters={'model_name': model_name, 'lr': lr, 'batch_size': batch_size}\n",
    "        gpu_parameters={'gpu_number':0, 'thread_number':1}\n",
    "        os.mkdir(output_folder)\n",
    "        data = Preprocessing(**preprocessing_parameters, input_length=input_length)\n",
    "        GPU_setup(**gpu_parameters)\n",
    "        model, history, training_time = Train_model(*tuple(data.values())[:4], **training_parameters, \n",
    "                                                    output_folder=output_folder, class_weight=class_weight)\n",
    "        Plot_training_curve(history)\n",
    "        results = Get_test_results(model, *tuple(data.values())[-2:])\n",
    "        Save_information(preprocessing_parameters, training_parameters, results, \n",
    "                         training_time, history, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_entry=[]\n",
    "for experiment in list(sorted([i for i in os.listdir() if 'output' in i])):\n",
    "    table_entry.append([experiment.replace('output_', '').replace('_', ', ')])\n",
    "    information=json.loads(open(experiment+'/'+'information.json', 'r').read().replace('\\n ', ', ')) \n",
    "    for key in ['accuracy ','precision','recall','f1_score']:\n",
    "        table_entry[-1].append(str(round(information[key]*100, 2)))\n",
    "        table_entry[-1][-1]+=''.join(['0' for i in range(5-len(table_entry[-1][-1]))])\n",
    "    for key in ['training_time']:\n",
    "        table_entry[-1].append(str(information[key]))\n",
    "\n",
    "results=(' \\\\\\\\ \\n').join([' & '.join(table_entry[i]) for i in range(len(table_entry))])\n",
    "results = '\\\\begin{table}[!ht] \\n \\centering \\n \\\\begin{tabular}{l|c c c c c } \\n \\hline \\n'+ \\\n",
    "' & '.join(['Experiment', 'Accuracy \\%', 'Precision \\%', 'Recall \\%', 'F1 \\%', 'Training Time (min)'])+ \\\n",
    "' \\\\\\\\ \\\\hline\\n'+results+' \\\\\\\\ \\\\hline\\n \\end{tabular} \\n \\caption{} \\n \\label{} \\n\\end{table}'\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history=pd.read_csv(output_folder+'history.csv')\n",
    "# Plot_training_curve(history)\n",
    "# information = Load_information(output_folder)\n",
    "# model = Load_model(output_folder)\n",
    "# results = Get_test_results(model, *tuple(data.values())[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_parameters={'auction':'NoAuction',\n",
    "                          'dataset_name':'DecPre',\n",
    "                          'y_type':'y2_true',\n",
    "                          'normalization_type':'sample-stationary',\n",
    "                          'split_type':'default',\n",
    "                         }\n",
    "data = Preprocessing(**preprocessing_parameters, input_length=100)\n",
    "gpu_parameters={'gpu_number':0, 'thread_number':1}\n",
    "GPU_setup(**gpu_parameters)\n",
    "model = Load_model('output_DeepLOB_sample-stationary/')\n",
    "inputs = Input(shape=data['train_x'].shape[1:])\n",
    "n_filters=16\n",
    "alpha,upper_threshold,lower_threshold=(0.01,None,0.0)\n",
    "layer1_out = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(\n",
    "    Conv2D(filters=n_filters, kernel_size=(1,2), strides=(1,2))(inputs))\n",
    "print(layer1_out.shape)\n",
    "layer2_out = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(\n",
    "    Conv2D(filters=n_filters, kernel_size=(4,1), padding='same')(layer1_out))\n",
    "print(layer2_out.shape)\n",
    "layer3_out = ReLU(max_value=upper_threshold, negative_slope=alpha, threshold=lower_threshold)(\n",
    "    Conv2D(filters=n_filters, kernel_size=(4,1), padding='same')(layer2_out))\n",
    "print(layer3_out.shape)\n",
    "model1 = keras.models.Model(inputs=inputs, outputs=[layer1_out,layer2_out,layer3_out])\n",
    "for i in range(len(model1.layers)):\n",
    "    model1.layers[i].set_weights(model.layers[i].get_weights())\n",
    "outputs = model1.predict(data['test_x'][range(0,len(data['test_x']),100)], batch_size=2048, verbose=1)\n",
    "print([i.shape for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=[]\n",
    "for output in outputs:\n",
    "    f.append(plt.figure(figsize=(16,10)))\n",
    "    _=f[-1].gca().violinplot(output.reshape(-1,16))\n",
    "    _=plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=f\n",
    "[i.gca().set_ylim([-0.5,5]) for i in g]\n",
    "g[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('NoAuction_DecPre.npy')\n",
    "mid_price = np.mean(data[:,[0,2]], axis=1)\n",
    "a=np.round(np.abs(mid_price-np.roll(mid_price, 1)), 15)\n",
    "half_tick=np.amin(a[a>0])\n",
    "print(half_tick, np.amin(mid_price), np.amax(mid_price), 100*half_tick/np.amax(mid_price), \n",
    "      0.5*100*half_tick/np.amax(mid_price), 0.2*100*half_tick/np.amax(mid_price))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
